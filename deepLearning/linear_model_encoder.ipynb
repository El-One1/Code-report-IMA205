{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = pd.read_csv('../metaDataTrain.csv')\n",
    "metaDataClean = metaData.iloc\n",
    "subject_list = load_training_dataset(metaDataClean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us construct the features vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros((100, 14))  ## there are 5 features and 20 subjects per training class\n",
    "\n",
    "features_metadata = select_row_x_and_y_from_table(metaDataClean, 2, 3)\n",
    "\n",
    "## adding into features vectors\n",
    "features[:,:2] = features_metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add volume information to feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes_0_ed = []\n",
    "volumes_1_ed = []\n",
    "volumes_2_ed = []\n",
    "volumes_3_ed = []\n",
    "\n",
    "volumes_0_es = []\n",
    "volumes_1_es = []\n",
    "volumes_2_es = []\n",
    "volumes_3_es = []\n",
    "   \n",
    "for i in range(0, 100):\n",
    "    subject = subject_list[i]\n",
    "    \n",
    "    ed_view = subject.ed_seg\n",
    "    volumes_0_ed.append(get_volume(ed_view, 0))\n",
    "    volumes_1_ed.append(get_volume(ed_view, 1))\n",
    "    volumes_2_ed.append(get_volume(ed_view, 2))\n",
    "    volumes_3_ed.append(get_volume(ed_view, 3))\n",
    "\n",
    "    es_view = subject.es_seg\n",
    "    volumes_0_es.append(get_volume(es_view, 0))\n",
    "    volumes_1_es.append(get_volume(es_view, 1))\n",
    "    volumes_2_es.append(get_volume(es_view, 2))\n",
    "    volumes_3_es.append(get_volume(es_view, 3))\n",
    "\n",
    "volumes_0_ed = np.array([volumes_0_ed]).flatten()\n",
    "volumes_1_ed = np.array([volumes_1_ed]).flatten() \n",
    "volumes_2_ed = np.array([volumes_2_ed]).flatten() \n",
    "volumes_3_ed = np.array([volumes_3_ed]).flatten() \n",
    "\n",
    "volumes_0_es = np.array([volumes_0_es]).flatten()\n",
    "volumes_1_es = np.array([volumes_1_es]).flatten() \n",
    "volumes_2_es = np.array([volumes_2_es]).flatten() \n",
    "volumes_3_es = np.array([volumes_3_es]).flatten() \n",
    "\n",
    "######### new features\n",
    "ejection_fraction_right = (volumes_1_ed - volumes_1_es) / volumes_1_ed\n",
    "ejection_fraction_left = (volumes_3_ed - volumes_3_es) / volumes_3_ed\n",
    "\n",
    "volumes_0_ed = np.array([volumes_0_ed]).flatten()\n",
    "volumes_1_ed = np.array([volumes_1_ed]).flatten() / volumes_0_ed\n",
    "volumes_2_ed = np.array([volumes_2_ed]).flatten() / volumes_0_ed\n",
    "volumes_3_ed = np.array([volumes_3_ed]).flatten() / volumes_0_ed\n",
    "\n",
    "volumes_0_es = np.array([volumes_0_es]).flatten()\n",
    "volumes_1_es = np.array([volumes_1_es]).flatten() / volumes_0_es\n",
    "volumes_2_es = np.array([volumes_2_es]).flatten() / volumes_0_es\n",
    "volumes_3_es = np.array([volumes_3_es]).flatten() / volumes_0_es\n",
    "\n",
    "ratioRL_ed = volumes_3_ed / volumes_1_ed\n",
    "ratioRL_es = volumes_3_es / volumes_1_es\n",
    "\n",
    "ratioML_ed = volumes_2_ed / volumes_1_ed\n",
    "ratioML_es = volumes_2_es / volumes_1_es\n",
    "\n",
    "\n",
    "\"\"\"for i, new_col in enumerate([ejection_fraction_right[:], volumes_1_ed[:], volumes_2_ed[:], volumes_3_ed[:], ejection_fraction_left[:], volumes_1_es[:], volumes_2_es[:], volumes_3_es[:]]):\n",
    "    features[:,i+2] = new_col\"\"\"\n",
    "\n",
    "for i, new_col in enumerate([ratioRL_ed[:], ratioRL_es[:], ratioML_ed[:], ratioML_es[:], ejection_fraction_right[:], volumes_1_ed[:], volumes_2_ed[:], volumes_3_ed[:], ejection_fraction_left[:], volumes_1_es[:], volumes_2_es[:], volumes_3_es[:]]):\n",
    "    features[:,i+2] = new_col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us just give our explicit class vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.zeros((100))\n",
    "classes[:20] = 2\n",
    "classes[20:40] = 3\n",
    "classes[40:60] = 1\n",
    "classes[60:80] = 0\n",
    "classes[80:100] = 4\n",
    "classes = np.array(classes.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elouan\\AppData\\Local\\Temp\\ipykernel_7908\\2690441315.py:5: FutureWarning: The input object of type 'ScalarImage' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'ScalarImage', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  X_train_ed = np.array([shaper(subject.ed_seg) for subject in subject_list])\n",
      "C:\\Users\\Elouan\\AppData\\Local\\Temp\\ipykernel_7908\\2690441315.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_ed = np.array([shaper(subject.ed_seg) for subject in subject_list])\n"
     ]
    }
   ],
   "source": [
    "shapes = np.array([subject.ed.shape for subject in subject_list])\n",
    "max1, max2, max3 = shapes[:,1].max(), shapes[:,2].max(), shapes[:,3].max()\n",
    "shaper = tio.CropOrPad((max1, max2, max3))\n",
    "\n",
    "X_train_ed = np.array([shaper(subject.ed_seg) for subject in subject_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv3d(64, 32, 3, padding=1)\n",
    "        self.conv5 = nn.Conv3d(32, 16, 3, padding=1)\n",
    "        self.conv6 = nn.Conv3d(16, 8, 3, padding=1)\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool3d(2, 2)\n",
    "        \n",
    "        self.denseim1 = nn.Linear(int(6656), 128)\n",
    "        self.denseim2 = nn.Linear(128, 16)\n",
    "\n",
    "        self.denseclass1 = nn.Linear(30, 12)\n",
    "        self.denseclass2 = nn.Linear(12, 5)\n",
    "\n",
    "        \"\"\"self.batchnorm16 = nn.BatchNorm3d(16)\n",
    "        self.batchnorm32 = nn.BatchNorm3d(32)\n",
    "        self.batchnorm64 = nn.BatchNorm3d(64)\"\"\"\n",
    "\n",
    "    def forward(self, im, stats, batchsize):\n",
    "\n",
    "        image_features = F.relu(self.conv1(im))\n",
    "\n",
    "        image_features = F.relu(self.conv2(image_features))\n",
    "        image_features = self.maxpool2(image_features)\n",
    "\n",
    "        image_features = F.relu(self.conv3(image_features))\n",
    "        image_features = self.maxpool2(image_features)\n",
    "\n",
    "        image_features = F.relu(self.conv4(image_features))\n",
    "        image_features = self.maxpool2(image_features)\n",
    "\n",
    "        image_features = F.relu(self.conv5(image_features))\n",
    "        image_features = self.maxpool2(image_features)\n",
    "\n",
    "        image_features = F.relu(self.conv6(image_features))\n",
    "\n",
    "        image_features = F.relu(self.denseim1(image_features.view(batchsize, -1)))\n",
    "        image_features = F.relu(self.denseim2(image_features))\n",
    "\n",
    "        features = torch.cat((image_features, stats), dim = 1)\n",
    "\n",
    "        features = F.relu(self.denseclass1(features))\n",
    "\n",
    "        classification = F.softmax(self.denseclass2(features), dim = 1)\n",
    "\n",
    "        return classification\n",
    "    \n",
    "class customds(Dataset):\n",
    "    def __init__(self, images, features, targets):\n",
    "        self.images = images\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        feature = self.features[index]\n",
    "        target = self.targets[index]\n",
    "        return image, feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.5\n",
    "batchsize = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.stack([X_train_ed[i].data for i in range(100)]).float()\n",
    "classes = torch.from_numpy(classes).float()\n",
    "classes.requires_grad = True\n",
    "features = torch.from_numpy(features).float()\n",
    "\n",
    "dataset = customds(images, features, classes)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "encoder(\n",
       "  (conv1): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv3): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv4): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv5): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv6): Conv3d(16, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (maxpool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (denseim1): Linear(in_features=6656, out_features=128, bias=True)\n",
       "  (denseim2): Linear(in_features=128, out_features=16, bias=True)\n",
       "  (denseclass1): Linear(in_features=30, out_features=12, bias=True)\n",
       "  (denseclass2): Linear(in_features=12, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_device(data, device):\n",
    "\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "        \n",
    "    return data.to(device, non_blocking = True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl \n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataloader = DeviceDataLoader(dataloader, device)\n",
    "to_device(model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_acc(predict,labels):\n",
    "  accuracy = torch.sum(predict == labels).item() / len(labels)\n",
    "  return accuracy\n",
    "\n",
    "def vector_to_class(x):\n",
    "  _, y = torch.max(x, dim = 1)\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.35 GiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 0 bytes free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Elouan\\Documents\\IMA205 Challenge\\deepLearning\\linear_model_encoder.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBatch number: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predict \u001b[39m=\u001b[39m model(batch_images, batch_features, batchsize)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# apply loss function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m prediction \u001b[39m=\u001b[39m vector_to_class(predict)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Elouan\\Documents\\IMA205 Challenge\\deepLearning\\linear_model_encoder.ipynb Cell 15\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, im, stats, batchsize):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     image_features \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(im))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     image_features \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(image_features))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Elouan/Documents/IMA205%20Challenge/deepLearning/linear_model_encoder.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool2(image_features)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.35 GiB (GPU 0; 4.00 GiB total capacity; 2.50 GiB already allocated; 0 bytes free; 2.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "i = 0\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "  train_loss=0.0\n",
    "  \n",
    "  for batch_images, batch_features, batch_targets in dataloader:\n",
    "\n",
    "    i+=1\n",
    "    print('Batch number: {}'.format(i))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predict = model(batch_images, batch_features, batchsize)\n",
    "    # apply loss function\n",
    "    prediction = vector_to_class(predict).float()\n",
    "    loss=criterion(prediction,batch_targets)\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss=loss.item()\n",
    "\n",
    "  print('Epoch:{} Train Loss:{:.4f}'.format(epoch+1,train_loss/batch_images.shape[0]))\n",
    "  # calculate accuracy\n",
    "\n",
    "  print('Accuracy:{:.2f}'.format(model_acc(prediction,batch_targets)))\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
